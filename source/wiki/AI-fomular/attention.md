---
title: attention 解析
date: 2026-02-05 12:00:00
---

这里才是正文...

# 缩放点积注意力

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

- $Q$ (Query): 你的查询意图。比如你手里拿着一张写着“哈利波特”的纸条。
- $K$ (Key): 图书的索引/标签。图书馆里每本书脊上贴的标签，比如“科幻”、“魔法”、“传记”。
- $V$ (Value): 书的内容本身。书里具体的文字信息。


这是一个极简的数值例子，模拟 Transformer 内部在处理两个单词时，是如何通过 **“缩放点积注意力” (Scaled Dot-Product Attention)** 计算出结果的。

## 场景设定

* **输入单词**：`Thinking` 和 `Machines`
* **目标**：计算第一个单词 **"Thinking"** 的注意力输出（即：它在结合了上下文 "Machines" 后，变成了什么样的新向量）。
* **参数假设**：为了方便计算，假设嵌入维度 $d_k = 2$。

---

## 0. 准备数据 ($Q, K, V$)

在真实模型中，$Q, K, V$ 是通过输入向量乘以权重矩阵 $W^Q, W^K, W^V$ 得到的。这里我们直接给出结果向量：

* **Query ($Q$)**: 代表 "Thinking" 去查询别人的意图。
    * $q_1 = [1, 2]$
* **Keys ($K$)**: 代表 "Thinking" 和 "Machines" 供别人查询的标签。
    * $k_1$ (Thinking) $= [1, 2]$
    * $k_2$ (Machines) $= [0, 1]$
* **Values ($V$)**: 代表这两个词实际包含的信息内容。
    * $v_1$ (Thinking) $= [10, 20]$
    * $v_2$ (Machines) $= [30, 40]$

---

## 1. 第一步：点积 ($QK^T$) —— 计算相似度

拿着 $q_1$ 去和所有的 $k$ 进行点积，看看 "Thinking" 和这句话里的每个词有多相关。

* **与自己点积** ($q_1 \cdot k_1$):
    $$1 \times 1 + 2 \times 2 = 1 + 4 = \mathbf{5}$$

* **与 "Machines" 点积** ($q_1 \cdot k_2$):
    $$1 \times 0 + 2 \times 1 = 0 + 2 = \mathbf{2}$$

**当前结果向量：** $[5, 2]$

> **💡 解读**：分数越高，代表相关性越高。Thinking 关注自己 5 分，关注 Machines 2 分。

---

## 2. 第二步：缩放 (Scaling)

公式要求除以 $\sqrt{d_k}$。因为我们设定 $d_k=4$ (为简化演示，此处假设根号后为2)，所以 $\sqrt{d_k}=2$。

$$
[5, 2] \div 2 = [\mathbf{2.5}, \mathbf{1.0}]
$$

**当前结果向量：** $[2.5, 1.0]$

> **💡 解读**：这一步是为了防止数值过大，导致下一步 Softmax 梯度消失。

---

## 3. 第三步：Softmax —— 归一化为概率

我们需要把分数变成百分比（权重），让它们的和为 1。公式如下：

$$
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum e^{x_j}}
$$

**计算过程：**
1. $e^{2.5} \approx 12.18$
2. $e^{1.0} \approx 2.71$
3. 总和 $= 12.18 + 2.71 = 14.89$

**计算权重：**
* $w_1$ (对自己): $12.18 / 14.89 \approx \mathbf{0.82}$
* $w_2$ (对 Machines): $2.71 / 14.89 \approx \mathbf{0.18}$

**当前注意力权重：** $[0.82, 0.18]$

> **💡 解读**：这意味着在理解 "Thinking" 这个词时，模型决定保留 **82%** 的“本意”，并混入 **18%** 的 "Machines" 的含义。

---

## 4. 第四步：加权求和 ($\dots \times V$)

最后，用计算出的权重，去混合 $V$ (实际内容)。

$$
\text{Output} = 0.82 \times v_1 + 0.18 \times v_2
$$

代入数据 ($v_1=[10, 20], v_2=[30, 40]$)：

* **第一维**：$0.82 \times 10 + 0.18 \times 30 = 8.2 + 5.4 = \mathbf{13.6}$
* **第二维**：$0.82 \times 20 + 0.18 \times 40 = 16.4 + 7.2 = \mathbf{23.6}$

### ✅ 最终结果
$$
\text{Attention Output} = [\mathbf{13.6}, \mathbf{23.6}]
$$

---

## 总结：这个结果意味着什么？

1. **原始的 Thinking ($v_1$)** 是 $[10, 20]$。
2. **处理后的 Thinking** 是 $[13.6, 23.6]$。

你可以看到，新的向量不再仅仅是 "Thinking" 自己了。它向 "Machines" ($[30, 40]$) 的方向偏移了一点点。

这就是 Attention 的本质：**它让 "Thinking" 这个词，吸收了 "Machines" 的一部分语义，变成了一个包含了上下文信息的“混合体”。**

> **🎨 调色盘比喻**
> * 本来 $V_1$ 是**纯红色**，$V_2$ 是**纯蓝色**。
> * Attention 告诉我们：取 **82% 的红**，加 **18% 的蓝**。
> * 最后输出的颜色：**带一点点紫色调的红色**。