---
title: 层归一化
date: 2026-02-05 12:00:00
---

这里才是正文...

# LayerNorm

公式全貌：
$$
\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta
$$

---

## 第一阶段：强制标准化 (Standardization)

$$
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

这一步的目的是**“消除差异，统一度量衡”**。

### 1. 符号拆解
* **$x$ (Input)**: 输入的向量。比如刚才算出来的 `[91, 71]`。
* **$\mu$ (Mean)**: **均值**。代表这个向量的“平均水平”。
    * *(91+71)/2 = 81*
* **$\sigma$ (Standard Deviation)**: **标准差**。代表这个向量里的数“波动有多大”。
    * *波动是 10*
* **$\epsilon$ (Epsilon)**: **极小值**（比如 0.00001）。
    * *作用*：防止分母为 0。如果方差是 0（所有数都一样），除法会报错，加上这个数保平安。

### 2. 物理意义：Z-Score (Z分数)
这其实就是统计学里最著名的 **Z-Score**。
* **$x - \mu$ (去中心化)**：把数据的中心强行拉回 **0**。
    * *不管你原来是 [91, 71] 还是 [10091, 10071]，减去均值后都变成 [10, -10]。*
* **除以 $\sigma$ (归一化)**：把数据的波动范围强行缩放到 **1**。
    * *不管你原来波动多大，除以标准差后，波动幅度都统一了。*

> **💡 举个例子：考试排名**
> * A班试卷简单，平均分 90 分。
> * B班试卷很难，平均分 50 分。
> * 这就好比 LayerNorm 之前的数据，量级不同，没法比较。
> * **Norm 之后**：大家都变成了“超过平均分多少个标准差”。如果你的得分是 +2.0，说明你是两个班里一样的学霸，无关试卷难易。

---

## 第二阶段：个性化调整 (Rescaling)

$$
y = \hat{x} \cdot \gamma + \beta
$$

这一步的目的是**“赋予模型自由”**。

### 1. 为什么标准化之后还要变回去？
如果我们只做第一步，所有的输出都会变成均值为 0、方差为 1 的死板数据。
**但是，神经网络可能不希望数据这么死板！**
也许对于某些任务，数据偏大一点（均值=5）或者波动大一点（方差=10）才更好？

### 2. 符号拆解
这两个参数不是算出来的，而是**模型自己“学”出来的**（就像权重矩阵 W 一样）。

* **$\gamma$ (Gamma - 缩放因子)**：
    * 模型会问：“标准差为 1 够吗？不够的话我乘个 2，把它拉宽。”
* **$\beta$ (Beta - 平移因子)**：
    * 模型会问：“均值为 0 合适吗？不合适的话我加个 5，把它整体抬高。”

> **💡 核心逻辑：可逆性**
> 如果模型发现“标准化”是个错误的决定，它完全可以通过学习让 $\gamma = \sigma$ 且 $\beta = \mu$，从而**把数据还原回去**。
>
> 这给了模型选择权：**它可以选择标准的数据，也可以选择保留原始特征。**