---
title: DeepSeek-R1-技术报告(未完成)
date: 2026-02-06 20:27:03
categories: 编程技术
tags: 编程
---
# DeepSeek R1

## 历史梳理

### Transformer

Transformer 模型虽然整体结构看着复杂，但如果从**数学公式的功能**来分类，其实可以非常清晰地划分为**四大类**。

如果把一个单词向量通过 Transformer 的过程比作**“刚入职员工的成长史”**，这四类公式的关系如下：

1. **位置编码 (准备)**：[注入灵魂](/wiki/AI-fomular/positional-encoding.html)
* *入职登记*。给你发工牌（位置信息），确定你在公司的工位（顺序）。
*  **(数据传给 Attention)**

2. **注意力机制 (交互)**：[理解上下文](/wiki/AI-fomular/attention.html) 
* *开会讨论*。你去和其他部门同事（其他 Token）沟通，了解大家都在干什么。你的脑子里装进了项目背景（上下文语义）。
*  **(数据传给 Add & Norm)**


3. **残差与归一化 (稳定)**：[层归一化](/wiki/AI-fomular/层归一化.html)
* *休息调整*。确保你没有因为开会太多而迷失自我（保留原始信息），并调整心态（归一化），准备下一项工作。
*  **(数据传给 FFN)**


4. **前馈网络 (变换)**：[前馈神经网络](/wiki/AI-fomular/前馈神经网络.html/)
* *独立思考*。回到工位，消化刚才开会的内容，结合自己的专业技能（权重矩阵），产出具体的方案。
*  **(数据再次经过 Add & Norm，进入下一层)**



**一句话总结前后关系：**
**“准备公式”**搭建舞台，**“交互公式”**负责收集情报，**变换公式**负责消化情报，而**稳定公式**贯穿全程，确保这套流程能循环 N 次而不崩塌。