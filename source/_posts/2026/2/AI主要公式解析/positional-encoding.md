# 实例解析：位置编码叠加公式

本例展示 Transformer 如何利用公式 $$X_{final} = X_{embedding} + PE_{(pos)}$$ 将位置信息注入到词向量中。

## 场景设定

* **输入句子**：`Thinking Machines`
* **目标对象**：第一个单词 **"Thinking"**。
* **位置 (pos)**：`0`（计算机从 0 开始计数）。
* **模型维度 ($d_{model}$)**：`4`（为了便于观察，假设维度只有 4 维）。

---

## 1. 拆解：$X_{embedding}$ (语义部分)

首先，模型在 Embedding 层查表，获取 "Thinking" 的原始语义向量。这个向量只代表单词的意思，不知道它在句子的哪个位置。

- Embedding 层: 这是 Transformer 的第一层参数，通常记作 $W_E$。你可以把它想象成一个巨大的矩阵。行数表示每个词，列数是模型的维度，一开始可能就是随机生成的。

假设查表结果为：
$$X_{\text{Thinking}} = [0.5, \quad 1.5, \quad -0.5, \quad 0.2]$$

> **含义**：这些数值代表了 "Thinking" 的特征（比如动词属性、抽象程度等）。

## 2. 拆解：$PE_{(pos)}$ (位置部分)

接下来，根据位置编码公式（基于 sin/cos），计算 **位置 0** 的编码向量。
根据公式，偶数维度用 $\sin$，奇数维度用 $\cos$。对于 $pos=0$：
* $\sin(0) = 0$
* $\cos(0) = 1$

所以，位置 0 的 $PE$ 向量是固定的：
$$PE_{(0)} = [0.0, \quad 1.0, \quad 0.0, \quad 1.0]$$

> **含义**：这个向量是“第1个座位”的专属身份证，与具体的词无关。任何放在开头的词，都会加上这个向量。

## 3. 执行：相加 ($X + PE$)

现在执行核心公式，将两者**元素对应相加**：

$$
\begin{aligned}
X_{final} &= X_{\text{Thinking}} + PE_{(0)} \\
&= [0.5, \quad 1.5, \quad -0.5, \quad 0.2] \quad (\text{语义}) \\
&+ [0.0, \quad 1.0, \quad 0.0, \quad 1.0] \quad (\text{位置}) \\
&= [\mathbf{0.5}, \quad \mathbf{2.5}, \quad \mathbf{-0.5}, \quad \mathbf{1.2}]
\end{aligned}
$$

---

## 4. 深度解读：为什么这样有效？

看着最终结果 $[\mathbf{0.5}, \mathbf{2.5}, \mathbf{-0.5}, \mathbf{1.2}]$，你可能会疑惑：原来的语义不是被破坏了吗？（比如 $1.5$ 变成了 $2.5$）。

我们可以用**“波形叠加”**来理解：

1. **数据的分离**：
    * Transformer 的高维空间非常大（通常 512 维）。
    * 神经网络非常聪明，它能学会识别：**“在这个向量里，某些特定的波动模式代表位置，而底层的数值代表含义。”**

2. **对比 "Machines" (Pos 1)**：
    如果我们要处理第二个词 **"Machines"**（假设它的语义向量也是 $[0.5, 1.5, \dots]$ 用于对比）：
    * 它会加上 $PE_{(1)}$（比如 $[0.84, 0.54, \dots]$）。
    * 结果就会完全不同。

3. **总结**：
    * **输入前**："Thinking" 只是字典里的一个词，没有时空概念。
    * **输出后 ($X_{final}$)**："Thinking" 变成了一个 **“坐在排头位置的 Thinking”**。
    * Attention 机制之后在计算时，就能利用这个微小的数值差异，区分出哪个是主语（通常在前），哪个是宾语（通常在后）。